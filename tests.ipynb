{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLP experimental framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./TLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Load and create datasets...\n",
      "\n",
      "=== Experiment 1/512 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [00:39<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Base Loss = 1.8058, Reg Loss = 0.0000, Total Loss = 1.8058 | Accuracy: 32.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 157/157 [00:39<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Base Loss = 1.4229, Reg Loss = 0.0000, Total Loss = 1.4229 | Accuracy: 47.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 157/157 [00:43<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Base Loss = 1.2833, Reg Loss = 0.0000, Total Loss = 1.2833 | Accuracy: 52.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [00:39<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Base Loss = 1.1476, Reg Loss = 0.0000, Total Loss = 1.1476 | Accuracy: 57.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  39%|███▉      | 62/157 [00:15<00:31,  3.05it/s]"
     ]
    }
   ],
   "source": [
    "# regularization\n",
    "# Decrese batch. Layer 2 only\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from copy import deepcopy\n",
    "\n",
    "# stage\n",
    "use_restored_weights = False\n",
    "inverse = False\n",
    "save_dir = f\"{root_dir}/raw_weights_stl10_3_7\"\n",
    "restore_dir = f\"{root_dir}/raw_weights_stl10_3_1\"\n",
    "method_description = (\n",
    "    \"STL10, 2 layer TLP. Regularisation. 'dirbias': True,'entropy': False,'var': True,'inverse': True,'lambda_dirbias': 0.1,'lambda_entropy': 0.01,'lambda_var': 0.1\"\n",
    ")\n",
    "if use_restored_weights:\n",
    "    method_description = \"STL10, 2 layer TLP. TLP frozen, retrain conv2\"\n",
    "\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device_type)\n",
    "device = torch.device(device_type)\n",
    "\n",
    "# Load STL-10 dataset\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(96, padding=4), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "if os.path.exists(\"data_cache/trainset.pkl\") and os.path.exists(\"data_cache/testset.pkl\"):\n",
    "    print(\"Load train data from cache...\")\n",
    "    with open(\"data_cache/trainset.pkl\", \"rb\") as f:\n",
    "        trainset = pickle.load(f)\n",
    "    with open(\"data_cache/testset.pkl\", \"rb\") as f:\n",
    "        testset = pickle.load(f)\n",
    "else:\n",
    "    print(\"Load and create datasets...\")\n",
    "    trainset = datasets.STL10(root=\"./data\", split=\"train\", download=True, transform=transform)\n",
    "    testset = datasets.STL10(root=\"./data\", split=\"test\", download=True, transform=transform)\n",
    "\n",
    "    # Сохраняем для последующих запусков\n",
    "    os.makedirs(\"data_cache\", exist_ok=True)\n",
    "    with open(\"data_cache/trainset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trainset, f)\n",
    "    with open(\"data_cache/testset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "def save_raw_weights(model, epoch, epochs, save_dir=\"raw_weights_stl10_2_2\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    weights = {}\n",
    "\n",
    "    if isinstance(model.pool2, TropicalMaxPool2d):\n",
    "        weights[\"pool2_weights\"] = model.pool2.weights.detach().cpu().numpy()\n",
    "\n",
    "    if hasattr(model, \"conv2\"):\n",
    "        weights[\"conv2_kernels\"] = model.conv2.weight.detach().cpu().numpy()\n",
    "\n",
    "    with open(os.path.join(save_dir, f\"epoch_{epoch + 1:02d}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "    if epoch + 1 == epochs:\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f\"epoch_{epoch + 1:02d}_state_dict.pt\"))\n",
    "        torch.save(model, os.path.join(save_dir, f\"epoch_{epoch + 1:02d}_full_model.pt\"))\n",
    "\n",
    "\n",
    "def log_experiment_results(uid, method_description, report_dict, runtime_seconds, test_loss, accuracy, save_dir, reg_opts=None):\n",
    "    LOG_FILE = f\"{root_dir}/log_TLP.jsonl\"\n",
    "    record = {\n",
    "        \"uid\": uid,\n",
    "        \"date\": datetime.now().isoformat(),\n",
    "        \"method\": method_description,\n",
    "        \"accuracy\": report_dict.get(\"accuracy\", None),\n",
    "        \"macro_f1\": report_dict.get(\"macro avg\", {}).get(\"f1-score\", None),\n",
    "        \"weighted_f1\": report_dict.get(\"weighted avg\", {}).get(\"f1-score\", None),\n",
    "        \"runtime_sec\": runtime_seconds,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"save_dir\": save_dir,\n",
    "        \"reg_opts\": reg_opts or {},\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "\n",
    "def load_tlp_weights(model, weights_path):\n",
    "    with open(weights_path, \"rb\") as f:\n",
    "        weights = pickle.load(f)\n",
    "    tlp_weights = torch.tensor(weights[\"pool2_weights\"])\n",
    "    with torch.no_grad():\n",
    "        model.pool2_tlp.weights.copy_(tlp_weights)\n",
    "    model.pool2_tlp.weights.requires_grad = False  # замораживаем\n",
    "\n",
    "\n",
    "def display_jsonl_as_table(sort_by=\"date\", descending=True):\n",
    "    LOG_FILE = f\"{root_dir}/log_TLP.jsonl\"\n",
    "    with open(LOG_FILE, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    df = pd.DataFrame([json.loads(line.strip()) for line in lines])\n",
    "    if sort_by in df.columns:\n",
    "        df = df.sort_values(by=sort_by, ascending=not descending)\n",
    "    # from IPython.display import display\n",
    "    # display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Learnable Tropical Pooling\n",
    "class TropicalMaxPool2d(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, stride=2, padding=0):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = nn.Parameter(torch.randn(channels, kernel_size * kernel_size) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x_padded = nn.functional.pad(x, (self.padding,) * 4)\n",
    "        unfolded = nn.functional.unfold(x_padded, kernel_size=self.kernel_size, stride=self.stride)\n",
    "        unfolded = unfolded.view(B, C, self.kernel_size * self.kernel_size, -1)\n",
    "        weighted = unfolded + self.weights.view(1, C, -1, 1)\n",
    "        pooled = weighted.max(dim=2)[0]\n",
    "        out_h = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        return pooled.view(B, C, out_h, out_w)\n",
    "\n",
    "\n",
    "# MaxPool wrapper\n",
    "class StandardMaxPool2d(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, stride=2, padding=0):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pool(x)\n",
    "\n",
    "\n",
    "# Shared architecture\n",
    "class ConvNetSTL10(nn.Module):\n",
    "    def __init__(self, pool_cls, in_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = pool_cls(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # After 3 × 2x2 poolings on 96×96 → 12×12\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = self.pool3(torch.relu(self.conv3(x)))\n",
    "        x = x.flatten(1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# === Регуляризация TLP ===\n",
    "def compute_alignment_regularization(\n",
    "    conv_feats, tlp_weights, reg_dirbias=True, reg_entropy=True, reg_var=True, inverse=False, lambda_dirbias=0.1, lambda_entropy=0.05, lambda_var=0.05\n",
    "):\n",
    "    reg_loss = 0.0\n",
    "    B, C, H, W = conv_feats.shape\n",
    "    conv_flat = conv_feats.view(B, C, -1).mean(-1)  # [B, C]\n",
    "    conv_norm = conv_flat / (conv_flat.norm(dim=1, keepdim=True) + 1e-6)  # [B, C]\n",
    "\n",
    "    tlp_norm = tlp_weights / (tlp_weights.norm(dim=1, keepdim=True) + 1e-6)  # [C, 4]\n",
    "    tlp_agg = tlp_norm.mean(dim=1)  # [C]\n",
    "    tlp_agg = tlp_agg / (tlp_agg.norm() + 1e-6)  # [C]\n",
    "\n",
    "    dot = (conv_norm * tlp_agg.unsqueeze(0)).sum(1)  # [B]\n",
    "\n",
    "    reg_components = []\n",
    "\n",
    "    if reg_dirbias:\n",
    "        bias = dot.mean()\n",
    "        reg_components.append(((-1 if inverse else 1) * lambda_dirbias * bias.abs()).clamp(min=0.0))\n",
    "\n",
    "    if reg_var:\n",
    "        var = dot.var()\n",
    "        reg_components.append(((-1 if inverse else 1) * lambda_var * var).clamp(min=0.0))\n",
    "\n",
    "    if reg_entropy:\n",
    "        p = torch.softmax(tlp_weights, dim=1)\n",
    "        entropy = -(p * torch.log(p + 1e-6)).sum(dim=1).mean()\n",
    "        reg_components.append(((-1 if inverse else 1) * lambda_entropy * entropy).clamp(min=0.0))\n",
    "\n",
    "    if reg_components:\n",
    "        reg_loss = sum(reg_components)\n",
    "\n",
    "    return reg_loss\n",
    "\n",
    "\n",
    "# === Метод для вытягивания признаков conv2 ===\n",
    "def extract_conv2_feats(model, x):\n",
    "    x = model.pool1(torch.relu(model.conv1(x)))\n",
    "    conv2 = torch.relu(model.conv2(x))\n",
    "    return conv2.detach()\n",
    "\n",
    "\n",
    "# Training and evaluation\n",
    "def train_model(model, loader, criterion, optimizer, epochs=5, save_weights=False, reg_opts=None):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total, correct, loss_sum, base_sum, reg_sum = 0, 0, 0.0, 0.0, 0.0\n",
    "        for inputs, labels in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            base_loss = criterion(outputs, labels)\n",
    "            loss = base_loss\n",
    "\n",
    "            if reg_opts is not None and hasattr(model.pool2, \"weights\"):\n",
    "                with torch.no_grad():\n",
    "                    conv_feats = extract_conv2_feats(model, inputs)\n",
    "                reg_loss = compute_alignment_regularization(\n",
    "                    conv_feats,\n",
    "                    model.pool2.weights,\n",
    "                    reg_dirbias=reg_opts.get(\"dirbias\", False),\n",
    "                    reg_entropy=reg_opts.get(\"entropy\", False),\n",
    "                    reg_var=reg_opts.get(\"var\", False),\n",
    "                    inverse=reg_opts.get(\"inverse\", False),\n",
    "                    lambda_dirbias=reg_opts.get(\"lambda_dirbias\", 0.1),\n",
    "                    lambda_entropy=reg_opts.get(\"lambda_entropy\", 0.05),\n",
    "                    lambda_var=reg_opts.get(\"lambda_var\", 0.05),\n",
    "                )\n",
    "                loss = base_loss + reg_loss\n",
    "                reg_sum += reg_loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "            base_sum += base_loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}: Base Loss = {base_sum / len(loader):.4f}, Reg Loss = {reg_sum / len(loader):.4f}, Total Loss = {loss_sum / len(loader):.4f} | Accuracy: {100 * correct / total:.2f}%\"\n",
    "        )\n",
    "\n",
    "        if save_weights:\n",
    "            save_raw_weights(model, epoch, epochs, save_dir=save_dir)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = outputs.max(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    report[\"accuracy\"] = accuracy\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {100 * accuracy:.2f}%\")\n",
    "    return report, avg_loss, accuracy\n",
    "\n",
    "\n",
    "import itertools\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Примерные базовые значения\n",
    "lambda_values = [0.0, 0.01, 0.05, 0.1]\n",
    "boolean_flags = [(True, False)]\n",
    "\n",
    "# Сетка параметров\n",
    "parameter_grid = list(\n",
    "    itertools.product(\n",
    "        boolean_flags[0],  # dirbias\n",
    "        boolean_flags[0],  # entropy\n",
    "        boolean_flags[0],  # var\n",
    "        lambda_values,  # lambda_dirbias\n",
    "        lambda_values,  # lambda_entropy\n",
    "        lambda_values,  # lambda_var\n",
    "    )\n",
    ")\n",
    "\n",
    "# Генерация конфигураций\n",
    "experiment_configs = []\n",
    "for dirbias, entropy, var, l_dirbias, l_entropy, l_var in parameter_grid:\n",
    "    config = {\"dirbias\": dirbias, \"entropy\": entropy, \"var\": var, \"inverse\": False, \"lambda_dirbias\": l_dirbias, \"lambda_entropy\": l_entropy, \"lambda_var\": l_var}\n",
    "    experiment_configs.append(config)\n",
    "\n",
    "\n",
    "# Пример логирования\n",
    "def log_experiment_results(uid, method_description, report_dict, runtime_seconds, test_loss, accuracy, save_dir, reg_opts=None):\n",
    "    LOG_FILE = f\"{root_dir}/log_TLP.jsonl\"\n",
    "    record = {\n",
    "        \"uid\": uid,\n",
    "        \"date\": datetime.now().isoformat(),\n",
    "        \"method\": method_description,\n",
    "        \"accuracy\": report_dict.get(\"accuracy\", None),\n",
    "        \"macro_f1\": report_dict.get(\"macro avg\", {}).get(\"f1-score\", None),\n",
    "        \"weighted_f1\": report_dict.get(\"weighted avg\", {}).get(\"f1-score\", None),\n",
    "        \"runtime_sec\": runtime_seconds,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"save_dir\": save_dir,\n",
    "        \"reg_opts\": reg_opts or {},\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "\n",
    "start_from = 0  # Last folder name\n",
    "for i, config in enumerate(experiment_configs):\n",
    "    if i < start_from:\n",
    "        continue\n",
    "    save_dir = f\"{root_dir}/raw_weights_stl10_3_8_{i}\"\n",
    "    print(f\"\\n=== Experiment {i+1}/{len(experiment_configs)} ===\")\n",
    "    uid = str(uuid.uuid4())\n",
    "    method_description = f\"ablation_TLP_{i:03d}\"\n",
    "\n",
    "    # Инициализация модели и оптимизатора\n",
    "    model = ConvNetSTL10(TropicalMaxPool2d).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Запуск обучения\n",
    "    start_time = time.time()\n",
    "    train_model(model, trainloader, criterion, optimizer, epochs=40, save_weights=True, reg_opts=deepcopy(config))\n",
    "    report_dict, test_loss, accuracy = evaluate_model(model, testloader, criterion)\n",
    "\n",
    "    # Логирование\n",
    "    log_experiment_results(\n",
    "        uid=uid,\n",
    "        method_description=method_description,\n",
    "        report_dict=report_dict,\n",
    "        runtime_seconds=round(time.time() - start_time, 2),\n",
    "        test_loss=test_loss,\n",
    "        accuracy=accuracy,\n",
    "        save_dir=save_dir,\n",
    "        reg_opts=config,\n",
    "    )\n",
    "\n",
    "\n",
    "display_jsonl_as_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
